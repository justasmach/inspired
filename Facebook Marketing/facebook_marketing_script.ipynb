{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...\n",
      "[{'since': '2019-10-01', 'until': '2019-10-07'}, {'since': '2019-10-08', 'until': '2019-10-14'}, {'since': '2019-10-15', 'until': '2019-10-21'}, {'since': '2019-10-22', 'until': '2019-10-28'}, {'since': '2019-10-29', 'until': '2019-10-31'}]\n",
      "Account ID: act_1212725268803451\n",
      "Calling Facebook Marketing API...\n",
      "Starting...\n",
      "[{'since': '2019-10-01', 'until': '2019-10-06'}, {'since': '2019-10-07', 'until': '2019-10-12'}, {'since': '2019-10-13', 'until': '2019-10-18'}, {'since': '2019-10-19', 'until': '2019-10-24'}, {'since': '2019-10-25', 'until': '2019-10-30'}, {'since': '2019-10-31', 'until': '2019-10-31'}]\n",
      "Account ID: act_1212725268803451\n",
      "Calling Facebook Marketing API...\n",
      "Starting...\n",
      "[{'since': '2019-10-01', 'until': '2019-10-05'}, {'since': '2019-10-06', 'until': '2019-10-10'}, {'since': '2019-10-11', 'until': '2019-10-15'}, {'since': '2019-10-16', 'until': '2019-10-20'}, {'since': '2019-10-21', 'until': '2019-10-25'}, {'since': '2019-10-26', 'until': '2019-10-30'}, {'since': '2019-10-31', 'until': '2019-10-31'}]\n",
      "Account ID: act_1212725268803451\n",
      "Calling Facebook Marketing API...\n",
      "Facebook marketing API Error\n",
      "\n",
      "\n",
      "  Message: Call was not successful\n",
      "  Method:  GET\n",
      "  Path:    https://graph.facebook.com/v4.0/act_1212725268803451/insights?access_token=EAAJw2gHFwssBABYEmUXh1L0AGIl2twkuZCkkJeTlVTjVLmvKN8R0rL86J4yQyXaVi1FJfRkiMZBPUnv1JXdP3LZB9yO6bPAXfW5GpQf03LjBduntbcZBbVsGwEl5ITMPEXBNBhLSdMgH4OtZAJJkEM79YTpTt1T5ZCeJrjMOapDwZDZD&fields=account_id%2Caccount_name%2Ccampaign_id%2Ccampaign_name%2Cadset_id%2Cadset_name%2Cdate_start%2Cdate_stop%2Cimpressions%2Cobjective%2Cclicks%2Cspend%2Creach%2Caction_values%2Cinline_link_clicks&filtering=%5B%5D&action_attribution_windows=%5B%271d_view%27%2C+%277d_view%27%2C+%2728d_view%27%2C+%271d_click%27%2C+%277d_click%27%2C+%2728d_click%27%2C+%27default%27%5D&breakdowns=%5B%27publisher_platform%27%2C+%27platform_position%27%5D&level=adset&time_increment=1&time_range=%7B%27since%27%3A+%272019-10-01%27%2C+%27until%27%3A+%272019-10-05%27%7D&limit=25&after=OTI0\n",
      "  Params:  {'time_range': \"{'since': '2019-10-01', 'until': '2019-10-05'}\", 'level': 'adset', 'filtering': '[]', 'action_attribution_windows': \"['1d_view', '7d_view', '28d_view', '1d_click', '7d_click', '28d_click', 'default']\", 'breakdowns': \"['publisher_platform', 'platform_position']\", 'time_increment': {1}, 'fields': 'account_id,account_name,campaign_id,campaign_name,adset_id,adset_name,date_start,date_stop,impressions,objective,clicks,spend,reach,action_values,inline_link_clicks'}\n",
      "\n",
      "  Status:  400\n",
      "  Response:\n",
      "    {\n",
      "      \"error\": {\n",
      "        \"message\": \"(#80000) There have been too many calls from this ad-account. Wait a bit and try again. For more info, please refer to https://developers.facebook.com/docs/graph-api/overview/rate-limiting.\",\n",
      "        \"type\": \"OAuthException\",\n",
      "        \"code\": 80000,\n",
      "        \"error_subcode\": 2446079,\n",
      "        \"fbtrace_id\": \"A62iUyHQB4c6t30Sd_eS-l_\"\n",
      "      }\n",
      "    }\n",
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "###### import everything upwards from home dir\n",
    "from __future__ import absolute_import\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "from configparser import ConfigParser\n",
    "import argparse\n",
    "import six\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "import json\n",
    "import yaml\n",
    "from xlrd import XLRDError\n",
    "from facebook_business.api import FacebookAdsApi\n",
    "from facebook_business.api import FacebookRequestError\n",
    "from facebook_business.adobjects.user import User\n",
    "from facebook_business.adobjects.adaccount import AdAccount\n",
    "from facebook_business.adobjects.campaign import Campaign\n",
    "from facebook_business.adobjects.adset import AdSet\n",
    "from facebookads.adobjects.adsinsights import AdsInsights\n",
    "import threading\n",
    "from ast import literal_eval\n",
    "from datetime import datetime, timedelta\n",
    "import io\n",
    "import helper_functions\n",
    "import google.ads.google_ads.client\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient import http\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.width', None)\n",
    "\n",
    "def facebook_marketing_api(account_id, df_conf_req, access_token, period): \n",
    "    \n",
    "    # create a list containing dimensions\n",
    "    breakdown_lst_call = []\n",
    "    dim_lst_call = []\n",
    "    for index, row in df_conf_req.iterrows():\n",
    "        if row['dimensions'] in ('publisher_platform', 'platform_position', 'action_attribution_windows'):\n",
    "            breakdown_lst_call.append(row['dimensions']) \n",
    "        else:\n",
    "            dim_lst_call.append(row['dimensions'])\n",
    "            \n",
    "    param_set_lst = {\n",
    "        'time_range': f\"{period}\",\n",
    "        #'date_preset': f\"{period}\",\n",
    "        'level': 'adset',\n",
    "        'filtering': [],\n",
    "        'action_attribution_windows': \"['1d_view', '7d_view', '28d_view', '1d_click', '7d_click', '28d_click', 'default']\",\n",
    "        'breakdowns': f\"{breakdown_lst_call}\",\n",
    "        'time_increment': {1}\n",
    "    }\n",
    "    \n",
    "    print('Calling Facebook Marketing API...')\n",
    "    FacebookAdsApi.init(access_token=access_token)\n",
    "    response = AdAccount(account_id).get_insights(fields = dim_lst_call, params = param_set_lst)\n",
    "    df_response = pd.DataFrame()\n",
    "    df_response_action = pd.DataFrame()\n",
    "    row_count = 0\n",
    "\n",
    "    var_lst = []\n",
    "    var_lst_action = []\n",
    "\n",
    "    for index, row in enumerate(response):\n",
    "        row_count = index\n",
    "        row_dict = vars(row)['_data']\n",
    "        var_dict_core = {}\n",
    "        for key in row_dict:\n",
    "            if key in ('account_id', 'campaign_id', 'adset_id', 'date_start', 'date_stop', 'objective', 'publisher_platform', 'platform_position'):\n",
    "                var_dict_core.update({key : row_dict[key]})\n",
    "        var_dict = {}\n",
    "        var_dict_action = {}\n",
    "        for key in row_dict:\n",
    "            if key == 'action_values':\n",
    "                action_values = row_dict['action_values']\n",
    "                if action_values:\n",
    "                    for key in action_values:\n",
    "                        var_dict_action.update(key)\n",
    "            else:\n",
    "                var_dict.update({key : row_dict[key]})\n",
    "        var_lst.append(var_dict)\n",
    "        if 'action_values' in str(row_dict.keys()):\n",
    "            if row_dict['action_values']:\n",
    "                var_dict_action.update(var_dict_core)\n",
    "                var_lst_action.append(var_dict_action)\n",
    "    if var_lst:\n",
    "        df_response = df_response.append(var_lst, ignore_index=True)\n",
    "    if var_lst_action:\n",
    "        df_response_action = df_response_action.append(var_lst_action, ignore_index=True)\n",
    "                          \n",
    "    print(str(row_count + 1) + ' row(s) received')\n",
    "    return df_response, df_response_action\n",
    "\n",
    "def db_config(filename='database.ini', section='postgresql'):\n",
    "    # create a parser\n",
    "    parser = ConfigParser()\n",
    "    # read config file\n",
    "    parser.read(filename)\n",
    " \n",
    "    # get section, default to postgresql\n",
    "    db = {}\n",
    "    if parser.has_section(section):\n",
    "        params = parser.items(section)\n",
    "        for param in params:\n",
    "            db[param[0]] = param[1]\n",
    "    else:\n",
    "        raise Exception('Section {0} not found in the {1} file'.format(section, filename)) \n",
    "    return db\n",
    "\n",
    "def create_table(tab_cr_str, conn, cur):\n",
    "    cur.execute(tab_cr_str)\n",
    "    conn.commit()\n",
    "    \n",
    "def tab_creation_str(t_name, pk_name, pk_lst, dtype_dict):\n",
    "    result = ''\n",
    "    columns = ''\n",
    "    for key in dtype_dict:\n",
    "        columns = columns + key + ' ' + dtype_dict[key] + ', '\n",
    "    query_str = (\"CREATE TABLE IF NOT EXISTS \" + t_name + \" \" + \n",
    "                \"(\" + columns + \"CONSTRAINT \" \n",
    "               + pk_name + \" PRIMARY KEY (\" + ', '.join(pk_lst) + \"));\")\n",
    "    return query_str, dtype_dict\n",
    "\n",
    "def init_conn():\n",
    "    conn = None\n",
    "    params = db_config()\n",
    "    conn = psycopg2.connect(**params)\n",
    "    return conn\n",
    "\n",
    "def rplc_nan(df_response):\n",
    "    df_response = df_response.replace({pd.np.nan: None})\n",
    "    return df_response\n",
    "\n",
    "def end_conn(conn):\n",
    "    conn.close()\n",
    "    \n",
    "def init_cur(conn):\n",
    "    cur = conn.cursor()\n",
    "    return cur\n",
    "    \n",
    "def add_column(t_name, db_dim, col_dtype, cur):\n",
    "    cur.execute(f\"ALTER TABLE {t_name} \"\n",
    "                f\"ADD COLUMN IF NOT EXISTS {db_dim} {col_dtype};\")\n",
    "\n",
    "def drop_table(t_name, do_drop, cur):\n",
    "    if do_drop:\n",
    "        cur.execute(f\"DROP TABLE IF EXISTS {t_name};\")\n",
    "    \n",
    "def add_ts(dataframe):\n",
    "    dataframe.insert(0, 'creation_ts', datetime.now())\n",
    "    dataframe.insert(0, 'last_updated_ts', datetime.now())\n",
    "    return dataframe \n",
    "\n",
    "def get_pln_no(df_response, src_col_name, is_in_df):\n",
    "    if is_in_df:\n",
    "        df_response.insert(0, 'pln_no', df_response[src_col_name].apply(pln_no_reg))\n",
    "    return df_response\n",
    "    \n",
    "def pln_no_reg(campaign_name):\n",
    "    return str(re.findall('(PLN?[\\-]\\d{1,4}?[\\-]\\d{1,4})', campaign_name))[2:-2]\n",
    "\n",
    "def rename_col(col_name):\n",
    "    if '.' in col_name:\n",
    "        return col_name.replace('.', '_')\n",
    "    elif col_name[0].isdigit():\n",
    "        return ('_' + col_name)\n",
    "    else:\n",
    "        return col_name\n",
    "    \n",
    "def get_types(df_response):   \n",
    "    col_names = df_response.head(0).to_dict()\n",
    "    col_dtypes = {}\n",
    "    dtype = ''\n",
    "    for key in col_names:\n",
    "            for i, row in df_response.iterrows():\n",
    "                try:\n",
    "                    dtype = ''\n",
    "                    if re.match('\\d{4}\\-\\d{2}\\-\\d{2}\\s+\\d{2}\\:\\d{2}\\:\\d{2}\\.\\d+', str(row[key])):\n",
    "                        dtype = 'TIMESTAMP'\n",
    "                        break\n",
    "                    elif re.match('\\d{4}\\-\\d{2}\\-\\d{2}', str(row[key])):\n",
    "                        dtype = 'DATE'\n",
    "                        break\n",
    "                    elif 'float' in str(type(literal_eval(row[key]))):\n",
    "                        dtype = 'REAL'\n",
    "                        break\n",
    "                    else:\n",
    "                        dtype = 'BIGINT'\n",
    "                except (ValueError, SyntaxError):\n",
    "                    dtype = 'VARCHAR'\n",
    "            col_dtypes.update({key : dtype})\n",
    "    return col_dtypes\n",
    "        \n",
    "def upsert_str(dims, t_name, pk):\n",
    "    upsert_q = (f\"INSERT INTO {t_name} ({dims}) \"\n",
    "                        \"VALUES %s \"\n",
    "                        f\"ON CONFLICT ({pk}) \" \n",
    "                            f\"DO \"\n",
    "                                f\"UPDATE \"\n",
    "                                f\"SET creation_ts = EXCLUDED.creation_ts; \")\n",
    "    return str(upsert_q)\n",
    "\n",
    "def upsert(conn, cur, upsert_q, df_res_tuple, page_size):\n",
    "    psycopg2.extras.execute_values(cur, upsert_q, df_res_tuple)\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "\n",
    "def postgre_write_main(df_response, t_name, pk_name, pk_lst, do_drop, page_size, src_col_name, is_pln_df):\n",
    "    df_response = df_response\n",
    "    t_name = t_name\n",
    "    pk_name = pk_name\n",
    "    pk_lst = pk_lst\n",
    "    do_drop = do_drop\n",
    "    page_size = page_size\n",
    "    src_col_name = src_col_name\n",
    "    is_pln_df = is_pln_df\n",
    "\n",
    "    try:\n",
    "        df_response = get_pln_no(df_response, src_col_name, is_pln_df)\n",
    "        df_response = rplc_nan(df_response)\n",
    "        df_response = df_response.rename(columns=rename_col)\n",
    "        df_response = add_ts(df_response)\n",
    "        dtype_dict = get_types(df_response)\n",
    "        conn = init_conn()\n",
    "        cur = init_cur(conn)\n",
    "        drop_table(t_name, do_drop, cur)\n",
    "        tab_creation = tab_creation_str(t_name, pk_name, pk_lst, dtype_dict)\n",
    "        tab_cr_str = tab_creation[0]\n",
    "        tab_types = tab_creation[1]\n",
    "        create_table(tab_cr_str, conn, cur)\n",
    "\n",
    "        df_res_tuple = list(df_response.itertuples(index=False, name=None))\n",
    "        upsert_q = upsert_str((','.join(dtype_dict.keys())), t_name, (','.join(pk_lst)))\n",
    "        upsert(conn, cur, upsert_q, df_res_tuple, page_size)    \n",
    "\n",
    "        end_conn(conn)\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print('Database error')\n",
    "        print(error)\n",
    "        sys.exit(1)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "    print('Database connection closed.')\n",
    "    print('')\n",
    "\n",
    "def period_split(def_period, def_intv):\n",
    "\n",
    "    start = datetime.strptime(list(def_period.values())[0],\"%Y-%m-%d\")\n",
    "    end = datetime.strptime(list(def_period.values())[1],\"%Y-%m-%d\")\n",
    "    diff = (end  - start ) / def_intv\n",
    "    period_lst = []\n",
    "    curr = start\n",
    "    period_lst.append(start.strftime(\"%Y-%m-%d\"))\n",
    "    while True:\n",
    "        curr = curr + timedelta(days = def_intv)\n",
    "        if curr >= end:\n",
    "            break\n",
    "        else:\n",
    "            period_lst.append(curr.strftime(\"%Y-%m-%d\"))\n",
    "    period_lst.append(end.strftime(\"%Y-%m-%d\"))\n",
    "    return period_lst\n",
    "    \n",
    "def facebook_marketing_prep(def_intv, account_id):\n",
    "\n",
    "    try:\n",
    "        print('Starting...')\n",
    "        try:\n",
    "            # read configuration from excel\n",
    "            df_conf_req = pd.read_excel('facebook_marketing_conf_1.xlsx', sheet_name='parameters', header=0)\n",
    "            def_period = dict(eval(df_conf_req.iat[0,1]))\n",
    "            db_config(filename = 'database.ini')\n",
    "            if pd.isna(df_conf_req['period'].iloc[0]):\n",
    "                raise KeyError('Period is missing')    \n",
    "            for index, row in df_conf_req.iterrows():\n",
    "                if pd.isna(row['dimensions']):\n",
    "                    raise KeyError('One or more dimensions missing')        \n",
    "        except(NameError, XLRDError, KeyError) as error:\n",
    "            print('Error while reading configuration file(s)')\n",
    "            print(error)\n",
    "            sys.exit(1)\n",
    "\n",
    "        with open(\"fb_secrets.yaml\", 'r') as secrets:\n",
    "            try:\n",
    "                secrets = yaml.safe_load(secrets)\n",
    "                app_id = str(secrets['app_id'])\n",
    "                app_secret = str(secrets['app_secret'])\n",
    "                access_token = str(secrets['access_token'])\n",
    "            except yaml.YAMLError as error:\n",
    "                print('Could not read FB secrets')\n",
    "                print(error)\n",
    "                sys.exit(1)\n",
    "        period_lst = period_split(def_period, def_intv)\n",
    "        per_dct_lst = []\n",
    "        for idx in range(len(period_lst) - 1):\n",
    "            start = period_lst[idx]\n",
    "            end = (datetime.strptime(period_lst[idx + 1], \"%Y-%m-%d\") - timedelta(days = 1)).strftime(\"%Y-%m-%d\")\n",
    "            if start > end:\n",
    "                period = {'since':f'{start}', 'until':f'{start}'}\n",
    "            else:\n",
    "                period = {'since':f'{start}', 'until':f'{end}'}\n",
    "            per_dct_lst.append(period)\n",
    "        print(per_dct_lst)\n",
    "        # iterate over customers            \n",
    "        try:\n",
    "            print('Account ID: ' + account_id)\n",
    "        except(KeyError) as error:\n",
    "            print('Could not read column')\n",
    "            print(error)\n",
    "            sys.exit(1)\n",
    "\n",
    "        for per_dct in per_dct_lst:\n",
    "            # call defined methods\n",
    "            df_response = pd.DataFrame()\n",
    "            df_response_action = pd.DataFrame()\n",
    "            period = per_dct\n",
    "            facebook_marketing_resp = facebook_marketing_api(account_id, df_conf_req, access_token, period)\n",
    "            df_response = facebook_marketing_resp[0]\n",
    "            df_response_action = facebook_marketing_resp[1]\n",
    "\n",
    "            t_name = 'facebook_marketing_temp_test'\n",
    "            pk_name = 'table_fb_pk_tst'\n",
    "            pk_lst = ['account_id', 'campaign_id', 'adset_id', 'date_start',\n",
    "                  'date_stop', 'objective', 'publisher_platform', 'platform_position']\n",
    "            do_drop = False\n",
    "            page_size = 1000\n",
    "            src_col_name = 'campaign_name'\n",
    "            is_pln_df = True\n",
    "\n",
    "            postgre_write_main(df_response, t_name, pk_name, pk_lst, do_drop, page_size, src_col_name, is_pln_df)\n",
    "            t_name = 'facebook_marketing_temp_action_test'\n",
    "            pk_name = 'table_fb_pk_action_tst'\n",
    "            src_col_name = ''\n",
    "            is_pln_df = False\n",
    "            postgre_write_main(df_response_action, t_name, pk_name, pk_lst, do_drop, page_size, src_col_name, is_pln_df)\n",
    "\n",
    "            print('Success')\n",
    "\n",
    "        return df_response, df_response_action\n",
    "    except(KeyError) as error:\n",
    "        print('Key error')\n",
    "        print(error)\n",
    "        sys.exit(1)\n",
    "    except(NameError) as error:\n",
    "        print('Name Error')\n",
    "        print(error)\n",
    "        sys.exit(1)\n",
    "    except(FacebookRequestError) as error:\n",
    "        if \"Please reduce the amount of data you're asking for\" in str(error):\n",
    "            if def_intv < 2:\n",
    "                print('day chunk size too large')\n",
    "                print(error)\n",
    "                sys.exit(1)\n",
    "            facebook_marketing_prep(def_intv - 1, account_id)\n",
    "        else:              \n",
    "            print('Facebook marketing API Error')\n",
    "            print(error)\n",
    "            sys.exit(1)\n",
    "            \n",
    "df_conf_base = pd.read_excel('facebook_marketing_conf_1.xlsx', sheet_name='base', header=0)\n",
    "def_intv = 7\n",
    "\n",
    "for index, row in df_conf_base.iterrows():  \n",
    "    try:\n",
    "        account_id = str(row['account_id'])\n",
    "        #account_id = str(df_conf_base.iat[0,0])\n",
    "        if pd.isna(df_conf_base['account_id'].iloc[0]):\n",
    "            raise KeyError('No base data provided (account_id(s))')\n",
    "        start = facebook_marketing_prep(def_intv, account_id)\n",
    "        df_response = start[0]\n",
    "        df_response_action = start[1]\n",
    "    except(KeyError) as error:\n",
    "        print(error)\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from ast import literal_eval\n",
    "import datetime\n",
    "def get_types(dataframe):   \n",
    "    col_names = df_response.head(0).to_dict()\n",
    "    col_dtypes = {}\n",
    "    dtype = ''\n",
    "    for key in col_names:\n",
    "            for i, row in df_response.iterrows():\n",
    "                try:\n",
    "                    if re.match('\\d{4}\\-\\d{2}\\-\\d{2}\\s+\\d{2}\\:\\d{2}\\:\\d{2}\\.\\d+', row[key]):\n",
    "                        dtype = 'TIMESTAMP'\n",
    "                        break\n",
    "                    elif re.match('\\d{4}\\-\\d{2}\\-\\d{2}', row[key]):\n",
    "                        dtype = 'DATE'\n",
    "                        break\n",
    "                    elif 'float' in str(type(literal_eval(row[key]))):\n",
    "                        dtype = 'REAL'\n",
    "                        break\n",
    "                    else:\n",
    "                        dtype = 'BIGINT'\n",
    "                except (ValueError, SyntaxError):\n",
    "                    dtype = 'VARCHAR'\n",
    "            col_dtypes.update({key : dtype})\n",
    "    return col_dtypes\n",
    "print(get_types(df_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_response['Position'].str.contains(\"PLN\")\n",
    "#df_response.filter(like=\"PLN\")\n",
    "print(df_response.columns[df_response.isin(['PLN']).any()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = datetime.date.today()\n",
    "date_list = [base - datetime.timedelta(days=x) for x in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(date_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
