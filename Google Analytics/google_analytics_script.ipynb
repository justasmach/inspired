{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:googleapiclient.discovery_cache:file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\justinas.maciulis\\Anaconda3\\lib\\site-packages\\googleapiclient\\discovery_cache\\__init__.py\", line 36, in autodetect\n",
      "    from google.appengine.api import memcache\n",
      "ModuleNotFoundError: No module named 'google.appengine'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\justinas.maciulis\\Anaconda3\\lib\\site-packages\\googleapiclient\\discovery_cache\\file_cache.py\", line 33, in <module>\n",
      "    from oauth2client.contrib.locked_file import LockedFile\n",
      "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\justinas.maciulis\\Anaconda3\\lib\\site-packages\\googleapiclient\\discovery_cache\\file_cache.py\", line 37, in <module>\n",
      "    from oauth2client.locked_file import LockedFile\n",
      "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\justinas.maciulis\\Anaconda3\\lib\\site-packages\\googleapiclient\\discovery_cache\\__init__.py\", line 41, in autodetect\n",
      "    from . import file_cache\n",
      "  File \"C:\\Users\\justinas.maciulis\\Anaconda3\\lib\\site-packages\\googleapiclient\\discovery_cache\\file_cache.py\", line 41, in <module>\n",
      "    'file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth')\n",
      "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2019-09-18', '2019-12-16')\n",
      "View ID: 160869820\n",
      "Batch 1\n",
      "2172 row(s) received\n",
      "Batch 2\n",
      "2172 row(s) received\n",
      "Batch 3\n",
      "931 row(s) received\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-151e892b40c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[0mlog_pltfrm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'google_analytics'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m     \u001b[0mga_prep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_pltfrm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;32mexcept\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mException\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[0mout_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Key Error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-151e892b40c7>\u001b[0m in \u001b[0;36mga_prep\u001b[1;34m(log_pltfrm)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[1;31m# call defined methods\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m         \u001b[0mgoogle_analytics_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgoogle_analytics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_conf_req\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mview_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey_file_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscopes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperiod_lst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_pltfrm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[0mt_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'google_analytics_new'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-151e892b40c7>\u001b[0m in \u001b[0;36mgoogle_analytics\u001b[1;34m(df_conf_req, view_id, key_file_location, scopes, period_lst, log_pltfrm)\u001b[0m\n\u001b[0;32m    101\u001b[0m                                 \u001b[0mdf_res_part\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmetricHeader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                             \u001b[0mvar_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mmetricHeader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m                     \u001b[0mdf_res_part\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_res_part\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[1;31m# if iteration is first and main dataframe is empty assign current response segment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mappend\u001b[1;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[0;32m   6690\u001b[0m         return concat(to_concat, ignore_index=ignore_index,\n\u001b[0;32m   6691\u001b[0m                       \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6692\u001b[1;33m                       sort=sort)\n\u001b[0m\u001b[0;32m   6693\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6694\u001b[0m     def join(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    227\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m                        copy=copy, sort=sort)\n\u001b[1;32m--> 229\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    424\u001b[0m             new_data = concatenate_block_managers(\n\u001b[0;32m    425\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m                 copy=self.copy)\n\u001b[0m\u001b[0;32m    427\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m                 \u001b[0mnew_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m   2056\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_uniform_join_units\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2057\u001b[0m             b = join_units[0].block.concat_same_type(\n\u001b[1;32m-> 2058\u001b[1;33m                 [ju.block for ju in join_units], placement=placement)\n\u001b[0m\u001b[0;32m   2059\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2060\u001b[0m             b = make_block(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mconcat_same_type\u001b[1;34m(self, to_concat, placement)\u001b[0m\n\u001b[0;32m    326\u001b[0m         \"\"\"\n\u001b[0;32m    327\u001b[0m         values = self._concatenator([blk.values for blk in to_concat],\n\u001b[1;32m--> 328\u001b[1;33m                                     axis=self.ndim - 1)\n\u001b[0m\u001b[0;32m    329\u001b[0m         return self.make_block_same_class(\n\u001b[0;32m    330\u001b[0m             values, placement=placement or slice(0, len(values), 1))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys, os, os.path\n",
    "base, tail = os.path.split(os.getcwd())\n",
    "sys.path.append(base)\n",
    "from helper_functions import *\n",
    "\n",
    "def google_analytics(df_conf_req, view_id, key_file_location, scopes, period_lst, log_pltfrm):\n",
    "    \n",
    "    \n",
    "    # Initializes an Analytics Reporting API V4 service object.\n",
    "    try:\n",
    "        credentials = ServiceAccountCredentials.from_json_keyfile_name(\n",
    "                key_file_location, scopes)\n",
    "    except(NameError, IOError, FileNotFoundError) as error:\n",
    "        out_str = 'Could not read configuration file(s)'\n",
    "        log_string(log_pltfrm, out_str)\n",
    "        print(error)\n",
    "        log_string(log_pltfrm, error)\n",
    "        sys.exit(1)   \n",
    "\n",
    "    # Build the service object.\n",
    "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
    "    \n",
    "    # define empty pandas dataframe\n",
    "    df_response = pd.DataFrame()\n",
    "    \n",
    "    # create lists for metrics and dimensions\n",
    "    dim_lst = []\n",
    "    met_lst = []\n",
    "    start_date = period_lst[0]\n",
    "    end_date = period_lst[1]\n",
    "    for index, req in df_conf_req.iterrows():\n",
    "        if not 'nan' in str(req['dimensions']):\n",
    "            dim_lst.append(dict({'name':req['dimensions']}))\n",
    "        if not 'nan' in str(req['metrics']):\n",
    "            met_lst.append(dict({'expression':req['metrics']}))  \n",
    "    \n",
    "    met_batches = list()\n",
    "    \n",
    "    # split metric list into batches, since with a single API call, a max of 10 metrics can be requested\n",
    "    i = 0\n",
    "    while True:\n",
    "        met_batch = list()\n",
    "        stop = 0\n",
    "        for index, met in enumerate(met_lst):\n",
    "            if len(met_batch) < 10 and i < len(met_lst):\n",
    "                res = met_batch.append(met_lst[i])\n",
    "                i=i+1\n",
    "        if len(met_batch) != 0:\n",
    "            met_batches.append(met_batch)\n",
    "        if i == len(met_lst):\n",
    "            x = True\n",
    "            break \n",
    "\n",
    "    # create empty dataframe for response segment\n",
    "    \n",
    "    out_str = 'Calling Google Analytics API...'\n",
    "    log_string(log_pltfrm, out_str)\n",
    "    try:\n",
    "        # iterate over metric batches\n",
    "        for index, batch in enumerate(met_batches):\n",
    "            \n",
    "            # define request body\n",
    "            body={\n",
    "            'reportRequests':\n",
    "            [{\n",
    "                'viewId': view_id,\n",
    "                'dateRanges': [{'startDate': start_date, 'endDate': end_date}],\n",
    "                'metrics': batch,\n",
    "                'dimensions': dim_lst,\n",
    "                'pageSize': 100000,\n",
    "                'includeEmptyRows': True}]}\n",
    "\n",
    "            # make the call to Google Analytics API\n",
    "            response = analytics.reports().batchGet(body=body).execute()\n",
    "\n",
    "            df_res_part = pd.DataFrame()\n",
    "            \n",
    "            # deconstruct JSON response\n",
    "            for report in response.get('reports', []):\n",
    "                columnHeader = report.get('columnHeader', {})\n",
    "                dimensionHeaders = columnHeader.get('dimensions', [])\n",
    "                metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
    "\n",
    "                # iterate over rows\n",
    "                for data_row in report.get('data', {}).get('rows', []):\n",
    "                    dimensions = data_row.get('dimensions', [])\n",
    "                    dateRangeValues = data_row.get('metrics', [])\n",
    "                    var_dict = {}\n",
    "\n",
    "                    #iterate over dimensions\n",
    "                    for header, dimension in zip(dimensionHeaders, dimensions):\n",
    "                        if header not in df_res_part:\n",
    "                            df_res_part[header] = pd.Series()\n",
    "                            df_res_part.astype({header: 'object'}).dtypes\n",
    "                        var_dict.update({header : str(dimension)})\n",
    "\n",
    "                    # iterate over metrics\n",
    "                    for i, values in enumerate(dateRangeValues):\n",
    "                        for metricHeader, value in zip(metricHeaders, values.get('values')):\n",
    "                            if metricHeader.get('name') not in df_res_part.columns:\n",
    "                                df_res_part[metricHeader.get('name')] = pd.Series()\n",
    "                            var_dict.update({metricHeader.get('name') : value})\n",
    "                    df_res_part = df_res_part.append(var_dict, ignore_index=True)\n",
    "        \n",
    "            # if iteration is first and main dataframe is empty assign current response segment\n",
    "            if df_response.empty:\n",
    "                df_response = df_res_part\n",
    "            # else do a left join and combine the two\n",
    "            elif not df_res_part.empty:\n",
    "                df_response = pd.merge(df_response, df_res_part,  how='left', on=['ga:campaign', 'ga:adcontent', 'ga:channelGrouping', 'ga:keyword', 'ga:date', 'ga:sourceMedium'])\n",
    "            row_count_part = len(df_res_part.index)\n",
    "            row_count_full = len(df_response.index)\n",
    "            out_str = ('Batch ' + str(index + 1))\n",
    "            print(out_str)\n",
    "            log_string(log_pltfrm, out_str)\n",
    "            out_str = (str(row_count_full) + ' row(s) received')\n",
    "            print(out_str)\n",
    "            log_string(log_pltfrm, out_str)\n",
    "        df_response['ga_viewid'] = view_id\n",
    "        \n",
    "    except(http.HttpError) as error:\n",
    "        print('GA API error')\n",
    "        print(out_str)\n",
    "        log_string(log_pltfrm, out_str)\n",
    "        print(error)  \n",
    "        log_string(log_pltfrm, error)\n",
    "        sys.exit(1)\n",
    "            \n",
    "    return df_response\n",
    "\n",
    "\n",
    "def ga_prep(log_pltfrm):\n",
    "    out_str = 'Starting...'\n",
    "    print(out_str)\n",
    "    log_string(log_pltfrm, out_str)\n",
    "    do_drop = False\n",
    "    try:\n",
    "        # read configuration from excel\n",
    "        df_conf_base = pd.read_excel('google_analytics_conf_1.xlsx', sheet_name='base', header=0)\n",
    "        df_conf_req = pd.read_excel('google_analytics_conf_1.xlsx', sheet_name='parameters', header=0)\n",
    "        if len(df_conf_base) == 0:\n",
    "            raise KeyError('No base data provided (view_id)')\n",
    "        if pd.isna(df_conf_req['dimensions'].iloc[0]):\n",
    "            for index, row in df_conf_req.iterrows():\n",
    "                if pd.isna(row['metrics']):\n",
    "                    raise KeyError('One or more metrics missing')        \n",
    "                if pd.isna(row['date_range']) and index < 2:\n",
    "                    raise KeyError('No date range provided')\n",
    "        per_format = \"lst\"\n",
    "        period_lst = []\n",
    "        period_lst.append(str(df_conf_req.iat[0,2]))\n",
    "        period_lst.append(str(df_conf_req.iat[1,2]))\n",
    "        period_lst = upd_last_90(period_lst, per_format)\n",
    "        \n",
    "        print(period_lst)\n",
    "        log_string(log_pltfrm, period_lst)\n",
    "        key_file_location = 'client_secrets.json'\n",
    "        scopes = ['https://www.googleapis.com/auth/analytics.readonly']\n",
    "    except(NameError, XLRDError, KeyError) as error:\n",
    "        out_str = ('Error while reading configuration file(s)')\n",
    "        print(out_str)\n",
    "        log_string(log_pltfrm, out_str)\n",
    "        print(error)\n",
    "        log_string(log_pltfrm, error)\n",
    "        sys.exit(1)      \n",
    "\n",
    "    # iterate over view IDs\n",
    "    for index, row in df_conf_base.iterrows():\n",
    "        try:\n",
    "            view_id = str(int(row['view_id']))\n",
    "            out_str = ('View ID: ' + view_id)\n",
    "            print(out_str)\n",
    "            log_string(log_pltfrm, out_str)\n",
    "        except(KeyError, ValueError) as error:\n",
    "            out_str = 'Could not read column'\n",
    "            print(out_str)\n",
    "            log_string(log_pltfrm, out_str)\n",
    "            print(error)\n",
    "            log_string(log_pltfrm, error)\n",
    "            sys.exit(1)\n",
    "        \n",
    "        # call defined methods\n",
    "        google_analytics_response = google_analytics(df_conf_req, view_id, key_file_location, scopes, period_lst, log_pltfrm)\n",
    "\n",
    "        t_name = 'google_analytics_new'\n",
    "        pk_name = 'ga_new_pk'\n",
    "        pk_lst = ['ga_viewID', 'ga_sourceMedium', 'ga_date', 'ga_campaign', 'ga_adcontent', 'ga_channelGrouping', 'ga_keyword']\n",
    "        page_size = 100000\n",
    "        src_col_name = 'ga_campaign'\n",
    "        is_pln_df = True\n",
    "        add_name_cl = True\n",
    "        \n",
    "        if not google_analytics_response.empty:\n",
    "            postgre_write_main(google_analytics_response, t_name, pk_name, pk_lst, do_drop, page_size, src_col_name, is_pln_df, log_pltfrm)\n",
    "            do_drop = False\n",
    "            out_str = ('Success')\n",
    "            print(out_str)\n",
    "            log_string(log_pltfrm, out_str)\n",
    "try:\n",
    "    log_pltfrm = 'google_analytics'\n",
    "    ga_prep(log_pltfrm)\n",
    "except KeyError as error:\n",
    "    out_str = ('Key Error')\n",
    "    print(out_str)\n",
    "    log_string(log_pltfrm, out_str)\n",
    "    print(error)\n",
    "    log_string(log_pltfrm, error)\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
