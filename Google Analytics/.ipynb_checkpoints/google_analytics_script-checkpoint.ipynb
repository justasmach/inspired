{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the PostgreSQL database...\n",
      "PostgreSQL database version:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-329a5cb49824>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[0mdb_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'database.ini'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m         \u001b[0mpostgre_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_response\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim_met_lst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mview_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-329a5cb49824>\u001b[0m in \u001b[0;36mpostgre_write\u001b[1;34m(df_response, dim_lst, view_id)\u001b[0m\n\u001b[0;32m    219\u001b[0m                                 \u001b[1;34mf\"UPDATE \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                                 f\"SET {upd_query}; \")\n\u001b[1;32m--> 221\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m        \u001b[1;31m# close the communication with the PostgreSQL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#import everything upwards from home dir\n",
    "from __future__ import absolute_import\n",
    "from googleapiclient.discovery import build\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from configparser import ConfigParser\n",
    "import psycopg2\n",
    "import argparse\n",
    "import six\n",
    "import sys\n",
    "import google.ads.google_ads.client\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "def get_report(df_conf_req, view_id, key_file_location, scopes):\n",
    "    \n",
    "    \n",
    "    # Initializes an Analytics Reporting API V4 service object.\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name(\n",
    "            key_file_location, scopes)\n",
    "\n",
    "    # Build the service object.\n",
    "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
    "    \n",
    "    # define empty pandas dataframe\n",
    "    df_response = pd.DataFrame()\n",
    "    \n",
    "    # create lists for metrics and dimensions\n",
    "    dim_lst = []\n",
    "    met_lst = []\n",
    "    start_date = df_conf_req.iat[0,2]\n",
    "    end_date = df_conf_req.iat[1,2]\n",
    "    for index, req in df_conf_req.iterrows():\n",
    "        if not 'nan' in str(req['dimensions']):\n",
    "            dim_lst.append(dict({'name':req['dimensions']}))\n",
    "        if not 'nan' in str(req['metrics']):\n",
    "            met_lst.append(dict({'expression':req['metrics']}))  \n",
    "    \n",
    "    dim_met_lst = dim_lst +  met_lst\n",
    "    \n",
    "    met_batches = list()\n",
    "    \n",
    "    # split metric list into batches, since a single API call can have a max of 10 metrics\n",
    "    i = 0\n",
    "    while True:\n",
    "        met_batch = list()\n",
    "        stop = 0\n",
    "        for index, met in enumerate(met_lst):\n",
    "            if len(met_batch) < 10 and i < len(met_lst):\n",
    "                res = met_batch.append(met_lst[i])\n",
    "                i=i+1\n",
    "        if len(met_batch) != 0:\n",
    "            met_batches.append(met_batch)\n",
    "        if i == len(met_lst):\n",
    "            x = True\n",
    "            break \n",
    "\n",
    "    # create empty dataframe for response segment\n",
    "    df_res_part = pd.DataFrame()\n",
    "    \n",
    "    # iterate over metric batches\n",
    "    for index, batch in enumerate(met_batches):\n",
    "        \n",
    "        # define request body\n",
    "        body={\n",
    "        'reportRequests': [\n",
    "        {\n",
    "            'viewId': VIEW_ID,\n",
    "            'dateRanges': [{'startDate': start_date, 'endDate': end_date}],\n",
    "            'metrics': batch,\n",
    "            'dimensions': dim_lst,\n",
    "            \n",
    "            'pageSize': 10000,\n",
    "            'includeEmptyRows': True\n",
    "\n",
    "        }]\n",
    "    }    \n",
    "        # make the call to Google Analytics API\n",
    "        response = analytics.reports().batchGet(body=body).execute()\n",
    "        \n",
    "        # deconstruct JSON response\n",
    "        for report in response.get('reports', []):\n",
    "            columnHeader = report.get('columnHeader', {})\n",
    "            dimensionHeaders = columnHeader.get('dimensions', [])\n",
    "            metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
    "\n",
    "            # iterate over rows\n",
    "            for data_row in report.get('data', {}).get('rows', []):\n",
    "                dimensions = data_row.get('dimensions', [])\n",
    "                dateRangeValues = data_row.get('metrics', [])\n",
    "                var_dict = {}\n",
    "                \n",
    "                #iterate over dimensions\n",
    "                for header, dimension in zip(dimensionHeaders, dimensions):\n",
    "                    if header not in df_res_part:\n",
    "                        df_res_part[header] = pd.Series()\n",
    "                        df_res_part.astype({header: 'object'}).dtypes\n",
    "                    var_dict.update({header : str(dimension)})\n",
    "                    \n",
    "                # iterate over metrics\n",
    "                for i, values in enumerate(dateRangeValues):\n",
    "                    for metricHeader, value in zip(metricHeaders, values.get('values')):\n",
    "                        if metricHeader.get('name') not in df_res_part.columns:\n",
    "                            df_res_part[metricHeader.get('name')] = pd.Series()\n",
    "                        var_dict.update({metricHeader.get('name') : value})\n",
    "                df_res_part = df_res_part.append(var_dict, ignore_index=True)\n",
    "        row_count_part = len(df_res_part.index)\n",
    "    # if iteration is first and main dataframe is empty assign current response segment\n",
    "    if df_response.empty:\n",
    "        df_response = df_res_part\n",
    "    # else do a left join and combine the two\n",
    "    else:\n",
    "        df_response = pd.merge(df_response, df_res_part,  how='left', left_on=['ga:campaign', 'ga:adcontent', 'ga:channelGrouping', 'ga:keyword', 'ga:date'], right_on = ['ga:campaign', 'ga:adcontent', 'ga:channelGrouping', 'ga:keyword', 'ga:date'])\n",
    "    row_count_full = len(df_response.index)\n",
    "    \n",
    "    return df_response, dim_met_lst\n",
    "\n",
    "def db_config(filename='database.ini', section='postgresql'):\n",
    "    # create a parser\n",
    "    parser = ConfigParser()\n",
    "    # read config file\n",
    "    parser.read(filename)\n",
    " \n",
    "    # get section, default to postgresql\n",
    "    db = {}\n",
    "    if parser.has_section(section):\n",
    "        params = parser.items(section)\n",
    "        for param in params:\n",
    "            db[param[0]] = param[1]\n",
    "    else:\n",
    "        raise Exception('Section {0} not found in the {1} file'.format(section, filename))\n",
    " \n",
    "    return db\n",
    "\n",
    "def postgre_write(df_response, dim_lst, view_id):\n",
    "    #Connect to the PostgreSQL database server\n",
    "    conn = None\n",
    "    # read connection parameters\n",
    "    params = db_config()\n",
    "    \n",
    "    # connect to the PostgreSQL server\n",
    "    print('Connecting to the PostgreSQL database...')\n",
    "    conn = psycopg2.connect(**params)\n",
    " \n",
    "    try:\n",
    "        # create a cursor\n",
    "        cur = conn.cursor()\n",
    "\n",
    "       # execute db write statements\n",
    "        print('PostgreSQL database version:')\n",
    "        # *tread lightly* cur.execute(\"DROP TABLE google_analytics_temp;\")\n",
    "        cur.execute(\"CREATE TABLE IF NOT EXISTS google_analytics_temp(\"\n",
    "                       \"ga_viewID bigint, \"\n",
    "                       \"ga_sourceMedium VARCHAR(200), \"\n",
    "                       \"ga_date DATE, \"\n",
    "                       \"ga_campaign VARCHAR(150), \"\n",
    "                       \"ga_adcontent VARCHAR(150), \"\n",
    "                       \"ga_channelGrouping VARCHAR(150), \"\n",
    "                       \"ga_keyword VARCHAR(150), \"\n",
    "                       \"CONSTRAINT table_analytics_pk PRIMARY KEY (ga_viewID, ga_sourceMedium, ga_date, ga_campaign, ga_adcontent, ga_channelGrouping, ga_keyword));\")\n",
    "        conn.commit()    \n",
    "        \n",
    "        # iterate over rows in response\n",
    "        for i, row in df_response.iterrows():\n",
    "            ins_query_dim ='ga_viewID, '\n",
    "            ins_query_val = f\"{view_id}, \"\n",
    "            upd_query = ''\n",
    "            col_dtype = ''\n",
    "            \n",
    "            # iterate over parameters in every row\n",
    "            for idx, dim in enumerate(list(row.index)):\n",
    "                db_dim = dim\n",
    "                val = str(row[dim])\n",
    "                \n",
    "                # replace DB illegal char with info string\n",
    "                uns_char_lst = [\"'\", \":\"]\n",
    "                for char in uns_char_lst:\n",
    "                    if char in str(row[dim]):\n",
    "                        val = val.replace(char, \"_uns_char_fnd_ascii_dec_expr_\" + str(ord(char)))\n",
    "                    if char in db_dim:\n",
    "                        db_dim = db_dim.replace(char, '_')\n",
    "                # conditionals for variable type assignment\n",
    "                if str(row[dim]) != 'nan':\n",
    "                    if str(db_dim) == 'ga_sessions':\n",
    "                        col_dtype = 'bigint'\n",
    "                        cur.execute(f\"ALTER TABLE google_analytics_temp \"\n",
    "                                    f\"ADD COLUMN IF NOT EXISTS {db_dim} {col_dtype};\")\n",
    "                        ins_query_dim = ins_query_dim + db_dim + ','\n",
    "                        ins_query_val = ins_query_val + \" '\" + val + \"',\"\n",
    "                        upd_query = upd_query + db_dim + ' = ' + val + ', '\n",
    "                        \n",
    "                    elif 'goal' in str(db_dim) or 'transaction' in str(db_dim):\n",
    "                        col_dtype = 'real'\n",
    "                        cur.execute(f\"ALTER TABLE google_analytics_temp \"\n",
    "                                    f\"ADD COLUMN IF NOT EXISTS {db_dim} {col_dtype};\")\n",
    "                        ins_query_dim = ins_query_dim + db_dim + ','\n",
    "                        ins_query_val = ins_query_val + \" '\" + val + \"',\"\n",
    "                        upd_query = upd_query + db_dim + ' = ' + val + ', '\n",
    "                    else:\n",
    "                        col_dtype = 'varchar (150)'\n",
    "                        cur.execute(f\"ALTER TABLE google_analytics_temp \"\n",
    "                                    f\"ADD COLUMN IF NOT EXISTS {db_dim} {col_dtype};\")\n",
    "                        ins_query_dim = ins_query_dim + db_dim + ','\n",
    "                        ins_query_val = ins_query_val + \" '\" + val + \"',\"\n",
    "            ins_query_dim = ins_query_dim[:-1]\n",
    "            ins_query_val = ins_query_val[:-1]\n",
    "            upd_query = upd_query[:-2]\n",
    "\n",
    "            # insert into db or if row exists update all metrics\n",
    "            cur.execute(f\"INSERT INTO google_analytics_temp ({ins_query_dim}) \"\n",
    "                        f\"VALUES ({ins_query_val}) \"\n",
    "                        f\"ON CONFLICT (ga_viewID, ga_sourceMedium, ga_date, ga_campaign, ga_adcontent, ga_channelGrouping, ga_keyword) \" \n",
    "                            f\"DO \"\n",
    "                                f\"UPDATE \"\n",
    "                                f\"SET {upd_query}; \")\n",
    "            conn.commit()\n",
    "       \n",
    "       # close the communication with the PostgreSQL\n",
    "        cur.close()\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "    print('Database connection closed.')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # read configuration from excel\n",
    "    df_conf_base = pd.read_excel('google_analytics_conf_1.xlsx', sheet_name='base', header=0)\n",
    "    df_conf_req = pd.read_excel('google_analytics_conf_1.xlsx', sheet_name='parameters', header=0) \n",
    "    \n",
    "    scopes = ['https://www.googleapis.com/auth/analytics.readonly']\n",
    "    key_file_location = 'client_secrets.json'\n",
    "    \n",
    "    # iterate over view IDs\n",
    "    for index, row in df_conf_base.iterrows():\n",
    "        view_id = str(row['view_id'])\n",
    "        #view_id = str(df_conf_base.iat[0,0])\n",
    "\n",
    "        # call defined methods\n",
    "        google_analytics_response = get_report(df_conf_req, view_id, key_file_location, scopes)\n",
    "        df_response = google_analytics_response[0]\n",
    "        dim_met_lst = google_analytics_response[1]\n",
    "\n",
    "        db_config(filename = 'database.ini')\n",
    "        postgre_write(df_response, dim_met_lst, view_id)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
