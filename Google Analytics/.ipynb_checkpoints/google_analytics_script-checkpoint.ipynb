{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...\n",
      "View ID: 166655749\n",
      "Calling Google Analytics API...\n",
      "Batch 1\n",
      "524 row(s) received\n",
      "Batch 2\n",
      "524 row(s) received\n",
      "Batch 3\n",
      "524 row(s) received\n",
      "Batch 4\n",
      "524 row(s) received\n",
      "Batch 5\n",
      "524 row(s) received\n",
      "Connecting to the PostgreSQL database...\n",
      "Working...\n",
      "524 row(s) inserted to DB\n",
      "Database connection closed.\n",
      "\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "#import everything upwards from home dir\n",
    "from __future__ import absolute_import\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient import http\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from configparser import ConfigParser\n",
    "import psycopg2\n",
    "import argparse\n",
    "import sys\n",
    "import google.ads.google_ads.client\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "from xlrd import XLRDError\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "def google_analytics(df_conf_req, view_id, key_file_location, scopes):\n",
    "    \n",
    "    \n",
    "    # Initializes an Analytics Reporting API V4 service object.\n",
    "    try:\n",
    "        credentials = ServiceAccountCredentials.from_json_keyfile_name(\n",
    "                key_file_location, scopes)\n",
    "    except(NameError, IOError, FileNotFoundError) as error:\n",
    "        print('Could not read configuration file(s)')\n",
    "        print(error)\n",
    "        sys.exit(1)   \n",
    "\n",
    "    # Build the service object.\n",
    "    analytics = build('analyticsreporting', 'v4', credentials=credentials)\n",
    "    \n",
    "    # define empty pandas dataframe\n",
    "    df_response = pd.DataFrame()\n",
    "    \n",
    "    # create lists for metrics and dimensions\n",
    "    dim_lst = []\n",
    "    met_lst = []\n",
    "    start_date = df_conf_req.iat[0,2]\n",
    "    end_date = df_conf_req.iat[1,2]\n",
    "    for index, req in df_conf_req.iterrows():\n",
    "        if not 'nan' in str(req['dimensions']):\n",
    "            dim_lst.append(dict({'name':req['dimensions']}))\n",
    "        if not 'nan' in str(req['metrics']):\n",
    "            met_lst.append(dict({'expression':req['metrics']}))  \n",
    "    \n",
    "    dim_met_lst = dim_lst +  met_lst\n",
    "    \n",
    "    met_batches = list()\n",
    "    \n",
    "    # split metric list into batches, since a single API call can have a max of 10 metrics\n",
    "    i = 0\n",
    "    while True:\n",
    "        met_batch = list()\n",
    "        stop = 0\n",
    "        for index, met in enumerate(met_lst):\n",
    "            if len(met_batch) < 10 and i < len(met_lst):\n",
    "                res = met_batch.append(met_lst[i])\n",
    "                i=i+1\n",
    "        if len(met_batch) != 0:\n",
    "            met_batches.append(met_batch)\n",
    "        if i == len(met_lst):\n",
    "            x = True\n",
    "            break \n",
    "\n",
    "    # create empty dataframe for response segment\n",
    "    \n",
    "    print('Calling Google Analytics API...')\n",
    "    \n",
    "    try:\n",
    "        # iterate over metric batches\n",
    "        for index, batch in enumerate(met_batches):\n",
    "            \n",
    "            # define request body\n",
    "            body={\n",
    "            'reportRequests': [\n",
    "            {\n",
    "                'viewId': view_id,\n",
    "                'dateRanges': [{'startDate': start_date, 'endDate': end_date}],\n",
    "                'metrics': batch,\n",
    "                'dimensions': dim_lst,\n",
    "\n",
    "                'pageSize': 10000,\n",
    "                'includeEmptyRows': True\n",
    "\n",
    "            }]\n",
    "            }    \n",
    "            # make the call to Google Analytics API\n",
    "            response = analytics.reports().batchGet(body=body).execute()\n",
    "            \n",
    "            df_res_part = pd.DataFrame()\n",
    "            \n",
    "            # deconstruct JSON response\n",
    "            for report in response.get('reports', []):\n",
    "                columnHeader = report.get('columnHeader', {})\n",
    "                dimensionHeaders = columnHeader.get('dimensions', [])\n",
    "                metricHeaders = columnHeader.get('metricHeader', {}).get('metricHeaderEntries', [])\n",
    "\n",
    "                # iterate over rows\n",
    "                for data_row in report.get('data', {}).get('rows', []):\n",
    "                    dimensions = data_row.get('dimensions', [])\n",
    "                    dateRangeValues = data_row.get('metrics', [])\n",
    "                    var_dict = {}\n",
    "\n",
    "                    #iterate over dimensions\n",
    "                    for header, dimension in zip(dimensionHeaders, dimensions):\n",
    "                        if header not in df_res_part:\n",
    "                            df_res_part[header] = pd.Series()\n",
    "                            df_res_part.astype({header: 'object'}).dtypes\n",
    "                        var_dict.update({header : str(dimension)})\n",
    "\n",
    "                    # iterate over metrics\n",
    "                    for i, values in enumerate(dateRangeValues):\n",
    "                        for metricHeader, value in zip(metricHeaders, values.get('values')):\n",
    "                            if metricHeader.get('name') not in df_res_part.columns:\n",
    "                                df_res_part[metricHeader.get('name')] = pd.Series()\n",
    "                            var_dict.update({metricHeader.get('name') : value})\n",
    "                    df_res_part = df_res_part.append(var_dict, ignore_index=True)\n",
    "        \n",
    "            # if iteration is first and main dataframe is empty assign current response segment\n",
    "            if df_response.empty:\n",
    "                df_response = df_res_part\n",
    "            # else do a left join and combine the two\n",
    "            else:\n",
    "                df_response = pd.merge(df_response, df_res_part,  how='inner', on=['ga:campaign', 'ga:adcontent', 'ga:channelGrouping', 'ga:keyword', 'ga:date', 'ga:sourceMedium'])\n",
    "            row_count_part = len(df_res_part.index)\n",
    "            row_count_full = len(df_response.index)\n",
    "            print('Batch ' + str(index + 1))\n",
    "            print(str(row_count_full) + ' row(s) received')\n",
    "\n",
    "    except(http.HttpError) as error:\n",
    "        print('API error')\n",
    "        print(error)  \n",
    "        sys.exit(1)\n",
    "            \n",
    "    return df_response, dim_met_lst\n",
    "\n",
    "def db_config(filename='database.ini', section='postgresql'):\n",
    "    # create a parser\n",
    "    parser = ConfigParser()\n",
    "    # read config file\n",
    "    parser.read(filename)\n",
    " \n",
    "    # get section, default to postgresql\n",
    "    db = {}\n",
    "    if parser.has_section(section):\n",
    "        params = parser.items(section)\n",
    "        for param in params:\n",
    "            db[param[0]] = param[1]\n",
    "    else:\n",
    "        raise Exception('Section {0} not found in the {1} file'.format(section, filename))\n",
    "    return db\n",
    "\n",
    "def postgre_write(df_response, dim_lst, view_id):\n",
    "    #Connect to the PostgreSQL database server\n",
    "    conn = None\n",
    "    # read connection parameters\n",
    "    params = db_config()\n",
    "    \n",
    "    # connect to the PostgreSQL server\n",
    "    print('Connecting to the PostgreSQL database...')\n",
    "    conn = psycopg2.connect(**params)\n",
    " \n",
    "    try:\n",
    "        # create a cursor\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # execute db write statements\n",
    "        # *tread lightly* cur.execute(\"DROP TABLE google_analytics_temp;\")\n",
    "        cur.execute(\"CREATE TABLE IF NOT EXISTS google_analytics_temp(\"\n",
    "                       \"creation_ts TIMESTAMP , \"\n",
    "                       \"last_updated_ts TIMESTAMP, \"\n",
    "                       \"ga_viewID bigint, \"\n",
    "                       \"ga_sourceMedium VARCHAR(200), \"\n",
    "                       \"ga_date DATE, \"\n",
    "                       \"ga_campaign VARCHAR(150), \"\n",
    "                       \"ga_adcontent VARCHAR(150), \"\n",
    "                       \"ga_channelGrouping VARCHAR(150), \"\n",
    "                       \"ga_keyword VARCHAR(150), \"\n",
    "                       \"CONSTRAINT table_analytics_pk PRIMARY KEY (ga_viewID, ga_sourceMedium, ga_date, ga_campaign, ga_adcontent, ga_channelGrouping, ga_keyword));\")\n",
    "        conn.commit()    \n",
    "        row_count = 0\n",
    "        print('Working...')\n",
    "        # iterate over rows in response\n",
    "        for i, row in df_response.iterrows():\n",
    "            row_count = i\n",
    "            creation_ts = datetime.datetime.now()\n",
    "            last_updated_ts = datetime.datetime.now()\n",
    "            \n",
    "            ins_query_dim ='ga_viewID, creation_ts, last_updated_ts, '\n",
    "            ins_query_val = f\"{view_id}, '{creation_ts}', '{last_updated_ts}', \"\n",
    "            upd_query = f\"last_updated_ts = '{last_updated_ts}', \"\n",
    "            col_dtype = ''\n",
    "            \n",
    "            # iterate over parameters in every row\n",
    "            for idx, dim in enumerate(list(row.index)):\n",
    "                db_dim = dim\n",
    "                val = str(row[dim])\n",
    "                \n",
    "                # replace DB illegal char with info string\n",
    "                uns_char_lst = [\"'\", \":\"]\n",
    "                for char in uns_char_lst:\n",
    "                    if char in str(row[dim]):\n",
    "                        val = val.replace(char, \"_uns_char_fnd_ascii_dec_expr_\" + str(ord(char)))\n",
    "                    if char in db_dim:\n",
    "                        db_dim = db_dim.replace(char, '_')\n",
    "                # conditionals for variable type assignment\n",
    "                if str(row[dim]) != 'nan':\n",
    "                    if str(db_dim) == 'ga_sessions':\n",
    "                        col_dtype = 'bigint'\n",
    "                        cur.execute(f\"ALTER TABLE google_analytics_temp \"\n",
    "                                    f\"ADD COLUMN IF NOT EXISTS {db_dim} {col_dtype};\")\n",
    "                        ins_query_dim = ins_query_dim + db_dim + ','\n",
    "                        ins_query_val = ins_query_val + \" '\" + val + \"',\"\n",
    "                        upd_query = upd_query + db_dim + ' = ' + val + ', '\n",
    "                        \n",
    "                    elif 'goal' in str(db_dim) or 'transaction' in str(db_dim):\n",
    "                        col_dtype = 'real'\n",
    "                        cur.execute(f\"ALTER TABLE google_analytics_temp \"\n",
    "                                    f\"ADD COLUMN IF NOT EXISTS {db_dim} {col_dtype};\")\n",
    "                        ins_query_dim = ins_query_dim + db_dim + ','\n",
    "                        ins_query_val = ins_query_val + \" '\" + val + \"',\"\n",
    "                        upd_query = upd_query + db_dim + ' = ' + val + ', '\n",
    "                    else:\n",
    "                        col_dtype = 'varchar (150)'\n",
    "                        cur.execute(f\"ALTER TABLE google_analytics_temp \"\n",
    "                                    f\"ADD COLUMN IF NOT EXISTS {db_dim} {col_dtype};\")\n",
    "                        ins_query_dim = ins_query_dim + db_dim + ','\n",
    "                        ins_query_val = ins_query_val + \" '\" + val + \"',\"\n",
    "            ins_query_dim = ins_query_dim[:-1]\n",
    "            ins_query_val = ins_query_val[:-1]\n",
    "            upd_query = upd_query[:-2]\n",
    "\n",
    "            # insert into db or if row exists update all metrics\n",
    "            cur.execute(f\"INSERT INTO google_analytics_temp ({ins_query_dim}) \"\n",
    "                        f\"VALUES ({ins_query_val}) \"\n",
    "                        f\"ON CONFLICT (ga_viewID, ga_sourceMedium, ga_date, ga_campaign, ga_adcontent, ga_channelGrouping, ga_keyword) \" \n",
    "                            f\"DO \"\n",
    "                                f\"UPDATE \"\n",
    "                                f\"SET {upd_query}; \")\n",
    "            conn.commit()\n",
    "        print(str(row_count + 1) + ' row(s) inserted to DB')\n",
    "       # close the communication with PostgreSQL\n",
    "        cur.close()\n",
    "    except (psycopg2.DatabaseError) as error:\n",
    "        print('Database error')\n",
    "        print(error)\n",
    "        sys.exit(1)\n",
    "    except(MemoryError) as error2:\n",
    "        print('Out of memory')\n",
    "        print(error2)\n",
    "        sys.exit(1)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "    print('Database connection closed.')\n",
    "    print('')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('Starting...')\n",
    "    try:\n",
    "        # read configuration from excel\n",
    "        df_conf_base = pd.read_excel('google_analytics_conf_1.xlsx', sheet_name='base', header=0)\n",
    "        df_conf_req = pd.read_excel('google_analytics_conf_1.xlsx', sheet_name='parameters', header=0)\n",
    "        if len(df_conf_base) == 0:\n",
    "            raise KeyError('No base data provided (view_id)')\n",
    "        if pd.isna(df_conf_req['dimensions'].iloc[0]):\n",
    "            raise KeyError('One or more dimensions missing')    \n",
    "        for index, row in df_conf_req.iterrows():\n",
    "            if pd.isna(row['metrics']):\n",
    "                raise KeyError('One or more metrics missing')        \n",
    "            if pd.isna(row['date_range']) and index < 2:\n",
    "                raise KeyError('No date range provided')\n",
    "        db_config(filename = 'database.ini')\n",
    "        key_file_location = 'client_secrets.json'\n",
    "        scopes = ['https://www.googleapis.com/auth/analytics.readonly']\n",
    "    except(NameError, XLRDError, KeyError) as error:\n",
    "        print('Error while reading configuration file(s)')\n",
    "        print(error)\n",
    "        sys.exit(1)      \n",
    "\n",
    "    # iterate over view IDs\n",
    "    for index, row in df_conf_base.iterrows():\n",
    "        try:\n",
    "            view_id = str(int(row['view_id']))\n",
    "            print('View ID: ' + view_id)\n",
    "        except(KeyError) as error:\n",
    "            print('Could not read column')\n",
    "            print(error)\n",
    "            sys.exit(1)\n",
    "        \n",
    "\n",
    "        # call defined methods\n",
    "        google_analytics_response = google_analytics(df_conf_req, view_id, key_file_location, scopes)\n",
    "        df_response = google_analytics_response[0]\n",
    "        dim_met_lst = google_analytics_response[1]\n",
    "\n",
    "        postgre_write(df_response, dim_met_lst, view_id)\n",
    "        print('Success')\n",
    "            \n",
    "          \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
